{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute if necessary\n",
    "# %%capture\n",
    "# !pip install numpy seaborn matplotlib pandas openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, Union, List\n",
    "import openml\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 5: Introducción a Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Instrucciones__: A continuación hay una lista de funciones que debe implementar o tareas que debe desarrollar. La descripción de cada una de ellas se encuentra en la definición de cada una de las funciones.\n",
    "\n",
    "La entrega de la práctica será en la siguiente sesión a menos que la indicación sea otra. La revisión iniciará al iniciar la sesión y únicamente podrá ser evaluada durante la duración de la sesión."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 1\n",
    "\n",
    "Implemente una clase  que permita ajustar un modelo KNN. La clase debe cumplir con los siguientes requisitos:\n",
    "\n",
    "- Debe contar con un método para realizar el entrenamiento en caso de ser requerido.\n",
    "- Debe contar con un método para realizar las predicciones.\n",
    "- Cada método debe imprimir una barra de progreso que permita conocer el tiempo estimado en que terminará.\n",
    "- La clase debe permitir el uso de la distancia Mikownski y similitud coseno."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo k-vecinos más cercanos (KNN) es un simple algoritmo de aprendizaje automático supervisado que se puede utilizar para resolver problemas de clasificación y regresión. Es fácil de implementar y comprender, pero tiene un inconveniente importante de volverse significativamente más lento a medida que crece el tamaño de los datos en uso.\n",
    "\n",
    "KNN trabaja buscando las distancias entre una consulta y todos los ejemplos en los datos, seleccionando el número especificado ejemplos (K) más cercanos a la consulta, luego vota por la etiqueta más frecuente (en el caso de la clasificación) o promedia las etiquetas (en el caso de la regresión).\n",
    "\n",
    "En el caso de la clasificación y la regresión, vimos esa elección la K correcta para nuestros datos se hace probando varios Ks y escogiendo el que mejor funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KNNModel:\n",
    "\n",
    "    def __init__(self, k=5, metric='euclidean' , p=2):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        start_time = time.time()\n",
    "        predictions = []\n",
    "        for x_test in tqdm(X):\n",
    "            distances = []\n",
    "            for x_train in self.X_train:\n",
    "\n",
    "                #identificamos el metodo de distancia a utilizar\n",
    "\n",
    "                if self.metric == 'euclidean':\n",
    "                    distance = np.sqrt(np.sum((x_test - x_train)**2))\n",
    "                elif self.metric == 'cosine':\n",
    "                    distance = 1 - (np.dot(x_test, x_train) / (np.linalg.norm(x_test) * np.linalg.norm(x_train)))\n",
    "                elif self.metric == 'manhattan':\n",
    "                    distance = np.sum(np.abs(x_test - x_train))\n",
    "                elif self.metric == 'mikownski':\n",
    "                    \n",
    "                    distance = (np.sum((x_test - x_train)**self.p))**(1/self.p)\n",
    "                else:\n",
    "                    raise ValueError('Metrica no soportada')\n",
    "\n",
    "                distances.append(distance)\n",
    "            #evaluamos la distancia de cada punto con el resto de los puntos\n",
    "            top_k_indices = np.argsort(distances)[:self.k]\n",
    "            top_k_classes = [self.y_train[i] for i in top_k_indices]\n",
    "            most_common_class = max(set(top_k_classes), key=top_k_classes.count)\n",
    "            predictions.append(most_common_class)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Prediccion completada en {elapsed_time:.2f} segundos.\")\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 601.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediccion completada en 0.20 segundos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 609.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediccion completada en 0.19 segundos.\n",
      "Precisión: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Cargar datos de ejemplo\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajustar modelo KNN\n",
    "model = KNNModel(k=5, metric='mikownski' , p=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular la puntuación en el conjunto de prueba\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "# Imprimir precisión\n",
    "print(f\"Precisión: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 2\n",
    "\n",
    "Implemente una clase que permita ajustar un modelo Naive Bayes. La clase debe cumplir con los siguientes requisitos:\n",
    "\n",
    "- Debe contar con un método para realizar el entrenamiento en caso de ser requerido.\n",
    "- Debe contar con un método para realizar las predicciones.\n",
    "- Cada método debe imprimir una barra de progreso que permita conocer el tiempo estimado en que terminará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se incluye un dataset real. El dataset importado se llama _spambase_ y puede leer su descripción en la siguiente liga\n",
    "\n",
    "https://www.openml.org/d/42904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga la metadata del dataset\n",
    "dataset_info = openml.datasets.get_dataset(42904, download_data=False)\n",
    "\n",
    "target = \"CLASS\"\n",
    "\n",
    "(\n",
    "    features, # Dataframe con las características que se pueden utilizar para predecir\n",
    "    outputs, # Columna a predecir\n",
    "    _, # Máscara que indica que columnas de todas las características son categoricas\n",
    "    columns # Lista con el nombre de las características\n",
    ")= dataset_info.get_data(\n",
    "    dataset_format=\"dataframe\", target=target\n",
    ")\n",
    "\n",
    "columns = np.array(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna a predecir se llama 'CLASS'\n",
      "Todas las características son ['COMMENT_ID' 'AUTHOR' 'DATE' 'CONTENT']\n"
     ]
    }
   ],
   "source": [
    "print(f\"La columna a predecir se llama '{target}'\")\n",
    "print(f\"Todas las características son {str(columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impresión de las características\n",
    "features\n",
    "# Eliminar las características que no se pueden utilizar para predecir\n",
    "features = features.drop(columns=[\"COMMENT_ID\", \"AUTHOR\", \"DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               CONTENT\n",
       "0    Huh, anyway check out this you[tube] channel: ...\n",
       "1    Hey guys check out my new channel and our firs...\n",
       "2               just for test I have to say murdev.com\n",
       "3     me shaking my sexy ass on my channel enjoy ^_^ ﻿\n",
       "4              watch?v=vtaRGgvGtWQ   Check this out .﻿\n",
       "..                                                 ...\n",
       "345  How can this have 2 billion views when there's...\n",
       "346         I don't now why I'm watching this in 2014﻿\n",
       "347  subscribe to me for call of duty vids and give...\n",
       "348  hi guys please my android photo editor downloa...\n",
       "349  The first billion viewed this because they tho...\n",
       "\n",
       "[350 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               CONTENT  CLASS\n",
      "0    huh anyway check out this youtube channel koby...      1\n",
      "1    hey guys check out my new channel and our firs...      1\n",
      "2                just for test i have to say murdevcom      1\n",
      "3        me shaking my sexy ass on my channel enjoy _       1\n",
      "4                  watchvvtarggvgtwq   check this out       1\n",
      "..                                                 ...    ...\n",
      "345  how can this have 2 billion views when theres ...      0\n",
      "346            i dont now why im watching this in 2014      0\n",
      "347  subscribe to me for call of duty vids and give...      1\n",
      "348  hi guys please my android photo editor downloa...      1\n",
      "349  the first billion viewed this because they tho...      0\n",
      "\n",
      "[350 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#unir en un solo dataframe las caracteristicas y la columna a predecir\n",
    "df = pd.concat([features, outputs], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "345    0\n",
       "346    0\n",
       "347    1\n",
       "348    1\n",
       "349    0\n",
       "Name: CLASS, Length: 350, dtype: uint8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_priors = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        start_time = time.time()\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = np.zeros(len(self.classes))\n",
    "        self.mean = np.zeros((len(self.classes), X.shape[1]))\n",
    "        self.var = np.zeros((len(self.classes), X.shape[1]))\n",
    "\n",
    "        # Calcular priori de clase\n",
    "        for i, c in enumerate(self.classes):\n",
    "            self.class_priors[i] = np.sum(y == c) / len(y)\n",
    "\n",
    "        # Calcular media y varianza de cada clase\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.mean[i] = X_c.mean(axis=0)\n",
    "            self.var[i] = X_c.var(axis=0)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Training completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        start_time = time.time()\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            class_probs = []\n",
    "            for i, c in enumerate(self.classes):\n",
    "                prior = np.log(self.class_priors[i])\n",
    "                posterior = np.sum(np.log(self.normal_pdf(x, self.mean[i], self.var[i])))\n",
    "                class_prob = prior + posterior\n",
    "                class_probs.append(class_prob)\n",
    "            predictions.append(self.classes[np.argmax(class_probs)])\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Prediction completed in {elapsed_time:.2f} seconds.\")\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def normal_pdf(self, x, mean, var):\n",
    "        eps = 1e-4\n",
    "        numerator = np.exp(-0.5 * (x - mean)**2 / (var + eps))\n",
    "        denominator = np.sqrt(2 * np.pi * var + eps)\n",
    "        return numerator / denominator\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba de Naive Bayes con el conjunto Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 0.00 seconds.\n",
      "Prediction completed in 0.00 seconds.\n",
      "Accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Cargar datos de ejemplo\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Ajustar modelo Naive Bayes\n",
    "model = NaiveBayes()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular la puntuación en el conjunto de prueba\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir precisión\n",
    "print(f\"Accuracy: {score:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio utilizando __nltk__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['CONTENT'] = features['CONTENT'].apply(lambda x: re.sub(r'http\\S+', '', x)) # Elimina enlaces\n",
    "features['CONTENT'] = features['CONTENT'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) # Elimina caracteres no deseados\n",
    "features['CONTENT'] = features['CONTENT'].apply(lambda x: x.lower()) # Convierte a minúsculas\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "features['CONTENT'] = features['CONTENT'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])) # Elimina stopwords\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "features['CONTENT'] = features['CONTENT'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()])) # Lematiza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [comment.split() for comment in features['CONTENT']]\n",
    "model = Word2Vec(corpus,  window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (350,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Convierte los comentarios en vectores numéricos usando Word2Vec\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray([np\u001b[39m.\u001b[39;49mmean([model\u001b[39m.\u001b[39;49mwv[word] \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m comment\u001b[39m.\u001b[39;49msplit() \u001b[39mif\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m model\u001b[39m.\u001b[39;49mwv], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m comment \u001b[39min\u001b[39;49;00m features[\u001b[39m'\u001b[39;49m\u001b[39mCONTENT\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (350,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Convierte los comentarios en vectores numéricos usando Word2Vec\n",
    "X = np.array([np.mean([model.wv[word] for word in comment.split() if word in model.wv], axis=0) for comment in features['CONTENT']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 3\n",
    "\n",
    "Realice el preprocesamiento que considere adecuado para que el dataset pueda ser procesado por un modelo de clasificación. __No olvide utilizar particionar en entrenamiento y validación, y realizar el preprocesamiento de manera adecuada__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 4\n",
    "\n",
    "Realice una visualización de los datos en dos dimensiones. Posteriormente entrene un modelo de KNN (utilizando la clase implementada anteriormente) y valide que su modelo generaliza bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 5\n",
    "\n",
    "Entrene un modelo de Naive Bayes (utilizando la clase implementada anteriormente) y valide que su modelo generaliza bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
