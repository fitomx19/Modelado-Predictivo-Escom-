{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute if necessary\n",
    "# %%capture\n",
    "# !pip install numpy seaborn matplotlib pandas openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, Union, List\n",
    "import openml\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 5: Introducción a Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Instrucciones__: A continuación hay una lista de funciones que debe implementar o tareas que debe desarrollar. La descripción de cada una de ellas se encuentra en la definición de cada una de las funciones.\n",
    "\n",
    "La entrega de la práctica será en la siguiente sesión a menos que la indicación sea otra. La revisión iniciará al iniciar la sesión y únicamente podrá ser evaluada durante la duración de la sesión."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 1\n",
    "\n",
    "Implemente una clase  que permita ajustar un modelo KNN. La clase debe cumplir con los siguientes requisitos:\n",
    "\n",
    "- Debe contar con un método para realizar el entrenamiento en caso de ser requerido.\n",
    "- Debe contar con un método para realizar las predicciones.\n",
    "- Cada método debe imprimir una barra de progreso que permita conocer el tiempo estimado en que terminará.\n",
    "- La clase debe permitir el uso de la distancia Mikownski y similitud coseno."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo k-vecinos más cercanos (KNN) es un simple algoritmo de aprendizaje automático supervisado que se puede utilizar para resolver problemas de clasificación y regresión. Es fácil de implementar y comprender, pero tiene un inconveniente importante de volverse significativamente más lento a medida que crece el tamaño de los datos en uso.\n",
    "\n",
    "KNN trabaja buscando las distancias entre una consulta y todos los ejemplos en los datos, seleccionando el número especificado ejemplos (K) más cercanos a la consulta, luego vota por la etiqueta más frecuente (en el caso de la clasificación) o promedia las etiquetas (en el caso de la regresión).\n",
    "\n",
    "En el caso de la clasificación y la regresión, vimos esa elección la K correcta para nuestros datos se hace probando varios Ks y escogiendo el que mejor funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KNNModel:\n",
    "\n",
    "    def __init__(self, k=5, metric='euclidean' , p=2):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        start_time = time.time()\n",
    "        predictions = []\n",
    "        for x_test in tqdm(X):\n",
    "            distances = []\n",
    "            for x_train in self.X_train:\n",
    "\n",
    "                #identificamos el metodo de distancia a utilizar\n",
    "\n",
    "                if self.metric == 'euclidean':\n",
    "                    distance = np.sqrt(np.sum((x_test - x_train)**2))\n",
    "                elif self.metric == 'cosine':\n",
    "                    distance = 1 - (np.dot(x_test, x_train) / (np.linalg.norm(x_test) * np.linalg.norm(x_train)))\n",
    "                elif self.metric == 'manhattan':\n",
    "                    distance = np.sum(np.abs(x_test - x_train))\n",
    "                elif self.metric == 'mikownski':\n",
    "                    \n",
    "                    distance = (np.sum((x_test - x_train)**self.p))**(1/self.p)\n",
    "                else:\n",
    "                    raise ValueError('Metrica no soportada')\n",
    "\n",
    "                distances.append(distance)\n",
    "            #evaluamos la distancia de cada punto con el resto de los puntos\n",
    "            top_k_indices = np.argsort(distances)[:self.k]\n",
    "            top_k_classes = [self.y_train[i] for i in top_k_indices]\n",
    "            most_common_class = max(set(top_k_classes), key=top_k_classes.count)\n",
    "            predictions.append(most_common_class)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Prediccion completada en {elapsed_time:.2f} segundos.\")\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 433.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediccion completada en 0.27 segundos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 440.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediccion completada en 0.26 segundos.\n",
      "Precisión: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Cargar datos de ejemplo\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajustar modelo KNN\n",
    "model = KNNModel(k=5, metric='mikownski' , p=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular la puntuación en el conjunto de prueba\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "# Imprimir precisión\n",
    "print(f\"Precisión: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 2\n",
    "\n",
    "Implemente una clase que permita ajustar un modelo Naive Bayes. La clase debe cumplir con los siguientes requisitos:\n",
    "\n",
    "- Debe contar con un método para realizar el entrenamiento en caso de ser requerido.\n",
    "- Debe contar con un método para realizar las predicciones.\n",
    "- Cada método debe imprimir una barra de progreso que permita conocer el tiempo estimado en que terminará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se incluye un dataset real. El dataset importado se llama _spambase_ y puede leer su descripción en la siguiente liga\n",
    "\n",
    "https://www.openml.org/d/42904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga la metadata del dataset\n",
    "dataset_info = openml.datasets.get_dataset(42904, download_data=False)\n",
    "\n",
    "target = \"CLASS\"\n",
    "\n",
    "(\n",
    "    features, # Dataframe con las características que se pueden utilizar para predecir\n",
    "    outputs, # Columna a predecir\n",
    "    _, # Máscara que indica que columnas de todas las características son categoricas\n",
    "    columns # Lista con el nombre de las características\n",
    ")= dataset_info.get_data(\n",
    "    dataset_format=\"dataframe\", target=target\n",
    ")\n",
    "\n",
    "columns = np.array(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna a predecir se llama 'CLASS'\n",
      "Todas las características son ['COMMENT_ID' 'AUTHOR' 'DATE' 'CONTENT']\n"
     ]
    }
   ],
   "source": [
    "print(f\"La columna a predecir se llama '{target}'\")\n",
    "print(f\"Todas las características son {str(columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impresión de las características\n",
    "features\n",
    "# Eliminar las características que no se pueden utilizar para predecir\n",
    "features = features.drop(columns=[\"COMMENT_ID\", \"AUTHOR\", \"DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               CONTENT\n",
       "0    Huh, anyway check out this you[tube] channel: ...\n",
       "1    Hey guys check out my new channel and our firs...\n",
       "2               just for test I have to say murdev.com\n",
       "3     me shaking my sexy ass on my channel enjoy ^_^ ﻿\n",
       "4              watch?v=vtaRGgvGtWQ   Check this out .﻿\n",
       "..                                                 ...\n",
       "345  How can this have 2 billion views when there's...\n",
       "346         I don't now why I'm watching this in 2014﻿\n",
       "347  subscribe to me for call of duty vids and give...\n",
       "348  hi guys please my android photo editor downloa...\n",
       "349  The first billion viewed this because they tho...\n",
       "\n",
       "[350 rows x 1 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def progressbar(it, prefix=\"\", size=60, file=sys.stdout):\n",
    "    count = len(it)\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        file.write(\"%s[%s%s] %i/%i\\r\" % (prefix, \"#\"*x, \".\"*(size-x), j, count))\n",
    "        file.flush()\n",
    "        file.write(\"\\n\")\n",
    "    show(0)\n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "        file.write(\"\\n\")\n",
    "    file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_priors = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "    def fit(self, X, y):\n",
    "        start_time = time.time()\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = np.zeros(len(self.classes))\n",
    "        self.mean = np.zeros((len(self.classes), X.shape[1]))\n",
    "        self.var = np.zeros((len(self.classes), X.shape[1]))\n",
    "        # Calcular priori de clase\n",
    "        for i, c in enumerate(self.classes):\n",
    "            self.class_priors[i] = np.sum(y == c) / len(y)\n",
    "        # Calcular media y varianza de cada clase\n",
    "        for i in progressbar(range(len(self.classes)), \"Procesando: \", 40):\n",
    "            for i, c in enumerate(self.classes):\n",
    "                X_c = X[y == c]\n",
    "                self.mean[i] = X_c.mean(axis=0)\n",
    "                self.var[i] = X_c.var(axis=0)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Entrenamiento completado en {elapsed_time:.2f} segundos.\")\n",
    "    def predict(self, X):\n",
    "        start_time = time.time()\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            for i in progressbar((X), \"Procesando: \", 40):\n",
    "                class_probs = []\n",
    "                for i, c in enumerate(self.classes):\n",
    "                    prior = np.log(self.class_priors[i])\n",
    "                    posterior = np.sum(np.log(self.normal_pdf(x, self.mean[i], self.var[i])))\n",
    "                    class_prob = prior + posterior\n",
    "                    class_probs.append(class_prob)\n",
    "                predictions.append(self.classes[np.argmax(class_probs)])\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Prediccion completada en  {elapsed_time:.2f} segundos.\")\n",
    "        return np.array(predictions)\n",
    "    def normal_pdf(self, x, mean, var):\n",
    "        eps = 1e-4\n",
    "        numerator = np.exp(-0.5 * (x - mean)**2 / (var + eps))\n",
    "        denominator = np.sqrt(2 * np.pi * var + eps)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio utilizando __nltk__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unir en un solo dataframe las caracteristicas y la columna a predecir\n",
    "data = pd.concat([features, outputs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               CONTENT  CLASS\n",
      "0          huh anyway check youtube channel kobyoshi02      1\n",
      "1    hey guy check new channel first vid u monkey i...      1\n",
      "2                                   test say murdevcom      1\n",
      "3                      shaking sexy as channel enjoy _      1\n",
      "4                              watchvvtarggvgtwq check      1\n",
      "..                                                 ...    ...\n",
      "345                    2 billion view there planet lol      0\n",
      "346                              dont im watching 2014      0\n",
      "347    subscribe call duty vids give aways goal100 sub      1\n",
      "348  hi guy please android photo editor download th...      1\n",
      "349  first billion viewed thought really cool billi...      0\n",
      "\n",
      "[350 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Separa los comentarios y etiquetas en dos arrays diferentes\n",
    "\n",
    "comentarios = data['CONTENT']\n",
    "etiquetas = data['CLASS']\n",
    "\n",
    "def preprocesar_comentarios(comentarios):\n",
    "    # Elimina caracteres no deseados\n",
    "    comentarios = comentarios.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    # Convierte a minúsculas\n",
    "    comentarios = comentarios.apply(lambda x: x.lower())\n",
    "    # Elimina stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    comentarios = comentarios.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    # Lematiza palabras\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    comentarios = comentarios.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "    return comentarios\n",
    "comentarios = preprocesar_comentarios(comentarios)\n",
    "#concatener comentarios + etiquetas en un solo dataframe\n",
    "data = pd.concat([comentarios, etiquetas], axis=1)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: [........................................] 0/2\n",
      "Procesando: [####################....................] 1/2\n",
      "\n",
      "Procesando: [########################################] 2/2\n",
      "\n",
      "Entrenamiento completado en 0.00 segundos.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def etiquetar_documentos(docs):\n",
    "    return [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(docs)]\n",
    "\n",
    "# Función para entrenar el modelo Doc2Vec\n",
    "def entrenar_doc2vec(docs):\n",
    "    docs_etiquetados = etiquetar_documentos(docs)\n",
    "    modelo = Doc2Vec(vector_size=300, min_count=3, epochs=40)\n",
    "    modelo.build_vocab(docs_etiquetados)\n",
    "    modelo.train(docs_etiquetados, total_examples=modelo.corpus_count, epochs=modelo.epochs)\n",
    "    return modelo\n",
    "\n",
    "modelo_doc2vec = entrenar_doc2vec(comentarios)\n",
    "def preprocesar_comentario(comentario):\n",
    "    tokens = word_tokenize(comentario.lower())\n",
    "    return modelo_doc2vec.infer_vector(tokens)\n",
    "comentarios_preprocesados = [preprocesar_comentario(comentario) for comentario in comentarios]\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(comentarios_preprocesados, etiquetas, test_size=0.2, random_state=42)\n",
    "# Crea una instancia de la clase NaiveBayes\n",
    "modelo = NaiveBayes()\n",
    "# Convierte los datos de entrenamiento a arrays NumPy\n",
    "X_entrenamiento = np.array(X_entrenamiento)\n",
    "y_entrenamiento = np.array(y_entrenamiento)\n",
    "# Entrena el modelo utilizando los datos de entrenamiento\n",
    "modelo.fit(X_entrenamiento, y_entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios = [\n",
    "    \"I love this song\",\n",
    "    \"Sub me now for free Android games, apps\",\n",
    "    \"Check my channel https://www.facebook.com/FUDAIRYQUEEN?pnref=story pls owo\",\n",
    "    \"Song name??\",\n",
    "    \"i hate this music. fucking singer and every koean chainise ana US sucks me dick.\",\n",
    "    \"https://www.facebook.com/FUDAIRYQUEEN?pnref=story\",\n",
    "    \"subscribe to me for call of duty vids and give aways Goal-100 subs﻿\",\n",
    "    \"WORLD RECORD YOUTUBE VIDEO VIEWS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: [........................................] 0/1\n",
      "Procesando: [########################################] 1/1\n",
      "\n",
      "Prediccion completada en  0.00 segundos.\n",
      "I love this song\n",
      "El comentario no es spam.\n",
      " \n",
      "Procesando: [........................................] 0/1\n",
      "Procesando: [########################################] 1/1\n",
      "\n",
      "Prediccion completada en  0.00 segundos.\n",
      "Sub me now for free Android games, apps\n",
      "El comentario no es spam.\n",
      " \n",
      "Procesando: [........................................] 0/1\n",
      "Procesando: [########################################] 1/1\n",
      "\n",
      "Prediccion completada en  0.00 segundos.\n",
      "Check my channel https://www.facebook.com/FUDAIRYQUEEN?pnref=story pls owo\n",
      "El comentario no es spam.\n",
      " \n",
      "Procesando: [........................................] 0/1\n",
      "Procesando: [########################################] 1/1\n",
      "\n",
      "Prediccion completada en  0.00 segundos.\n",
      "Song name??\n",
      "El comentario no es spam.\n",
      " \n",
      "Procesando: [........................................] 0/1\n",
      "Procesando: [########################################] 1/1\n",
      "\n",
      "Prediccion completada en  0.00 segundos.\n",
      "i hate this music. fucking singer and every koean chainise ana US sucks me dick.\n",
      "El comentario no es spam.\n",
      " \n",
      "Procesando: [........................................] 0/1\n",
      "Procesando: [########################################] 1/1\n",
      "\n",
      "Prediccion completada en  0.00 segundos.\n",
      "https://www.facebook.com/FUDAIRYQUEEN?pnref=story\n",
      "El comentario es spam.\n",
      " \n",
      "Procesando: [........................................] 0/1\n",
      "Procesando: [########################################] 1/1\n",
      "\n",
      "Prediccion completada en  0.00 segundos.\n",
      "subscribe to me for call of duty vids and give aways Goal-100 subs﻿\n",
      "El comentario no es spam.\n",
      " \n",
      "Procesando: [........................................] 0/1\n",
      "Procesando: [########################################] 1/1\n",
      "\n",
      "Prediccion completada en  0.00 segundos.\n",
      "WORLD RECORD YOUTUBE VIDEO VIEWS\n",
      "El comentario no es spam.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Predice si un nuevo comentario es spam o no\n",
    "from IPython.display import Markdown    \n",
    "resultados = []\n",
    "for com in comentarios:\n",
    "    nuevo_comentario_preprocesado = preprocesar_comentario(com)\n",
    "    es_spam = modelo.predict([nuevo_comentario_preprocesado])\n",
    "    # Muestra si el nuevo comentario es spam o no\n",
    "    if es_spam[0] == 1:\n",
    "        print(com)\n",
    "        print(\"El comentario es spam.\")\n",
    "        print(\" \")\n",
    "    else:\n",
    "        print(com)\n",
    "        print(\"El comentario no es spam.\")\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 3\n",
    "\n",
    "Realice el preprocesamiento que considere adecuado para que el dataset pueda ser procesado por un modelo de clasificación. __No olvide utilizar particionar en entrenamiento y validación, y realizar el preprocesamiento de manera adecuada__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga la metadata del dataset\n",
    "dataset_info = openml.datasets.get_dataset(42904, download_data=False)\n",
    "\n",
    "target = \"CLASS\"\n",
    "\n",
    "(\n",
    "    features, # Dataframe con las características que se pueden utilizar para predecir\n",
    "    outputs, # Columna a predecir\n",
    "    _, # Máscara que indica que columnas de todas las características son categoricas\n",
    "    columns # Lista con el nombre de las características\n",
    ")= dataset_info.get_data(\n",
    "    dataset_format=\"dataframe\", target=target\n",
    ")\n",
    "\n",
    "columns = np.array(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>z13th1q4yzihf1bll23qxzpjeujterydj</td>\n",
       "      <td>Carmen Racasanu</td>\n",
       "      <td>2014-11-14T13:27:52</td>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k</td>\n",
       "      <td>diego mogrovejo</td>\n",
       "      <td>2014-11-14T13:28:08</td>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>z130zd5b3titudkoe04ccbeohojxuzppvbg</td>\n",
       "      <td>BlueYetiPlayz -Call Of Duty and More</td>\n",
       "      <td>2015-05-23T13:04:32</td>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>z12he50arvrkivl5u04cctawgxzkjfsjcc4</td>\n",
       "      <td>Photo Editor</td>\n",
       "      <td>2015-06-05T14:14:48</td>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k</td>\n",
       "      <td>Ray Benich</td>\n",
       "      <td>2015-06-05T18:05:16</td>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID  \\\n",
       "0    LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU   \n",
       "1    LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A   \n",
       "2    LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8   \n",
       "3            z13jhp0bxqncu512g22wvzkasxmvvzjaz04   \n",
       "4            z13fwbwp1oujthgqj04chlngpvzmtt3r3dw   \n",
       "..                                           ...   \n",
       "345            z13th1q4yzihf1bll23qxzpjeujterydj   \n",
       "346        z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k   \n",
       "347          z130zd5b3titudkoe04ccbeohojxuzppvbg   \n",
       "348          z12he50arvrkivl5u04cctawgxzkjfsjcc4   \n",
       "349        z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k   \n",
       "\n",
       "                                   AUTHOR                 DATE  \\\n",
       "0                               Julius NM  2013-11-07T06:20:48   \n",
       "1                             adam riyati  2013-11-07T12:37:15   \n",
       "2                        Evgeny Murashkin  2013-11-08T17:34:21   \n",
       "3                         ElNino Melendez  2013-11-09T08:28:43   \n",
       "4                                  GsMega  2013-11-10T16:05:38   \n",
       "..                                    ...                  ...   \n",
       "345                       Carmen Racasanu  2014-11-14T13:27:52   \n",
       "346                       diego mogrovejo  2014-11-14T13:28:08   \n",
       "347  BlueYetiPlayz -Call Of Duty and More  2015-05-23T13:04:32   \n",
       "348                          Photo Editor  2015-06-05T14:14:48   \n",
       "349                            Ray Benich  2015-06-05T18:05:16   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "0    Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1    Hey guys check out my new channel and our firs...      1  \n",
       "2               just for test I have to say murdev.com      1  \n",
       "3     me shaking my sexy ass on my channel enjoy ^_^ ﻿      1  \n",
       "4              watch?v=vtaRGgvGtWQ   Check this out .﻿      1  \n",
       "..                                                 ...    ...  \n",
       "345  How can this have 2 billion views when there's...      0  \n",
       "346         I don't now why I'm watching this in 2014﻿      0  \n",
       "347  subscribe to me for call of duty vids and give...      1  \n",
       "348  hi guys please my android photo editor downloa...      1  \n",
       "349  The first billion viewed this because they tho...      0  \n",
       "\n",
       "[350 rows x 5 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unir en un solo dataframe las caracteristicas y la columna a predecir\n",
    "data = pd.concat([features, outputs], axis=1)\n",
    "#pasar a un dataframe \n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adolfo\\AppData\\Local\\Temp\\ipykernel_8508\\2966016472.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['CONTENT'] = data['CONTENT'].str.replace('[^\\w\\s]', '')  # Eliminar puntuación\n",
      "C:\\Users\\Adolfo\\AppData\\Local\\Temp\\ipykernel_8508\\2966016472.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['CONTENT'] = data['CONTENT'].str.replace('\\d+', '')  # Eliminar números\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Preprocesamiento de texto\n",
    "data['CONTENT'] = data['CONTENT'].str.lower()  # Convertir a minúsculas\n",
    "data['CONTENT'] = data['CONTENT'].str.replace('[^\\w\\s]', '')  # Eliminar puntuación\n",
    "data['CONTENT'] = data['CONTENT'].str.replace('\\d+', '')  # Eliminar números\n",
    "\n",
    "# Codificar las etiquetas de clase como valores numéricos\n",
    "le = LabelEncoder()\n",
    "data['CLASS'] = le.fit_transform(data['CLASS'])\n",
    "\n",
    "# Dividir los datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['CONTENT'], data['CLASS'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorización de características\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 4\n",
    "\n",
    "Realice una visualización de los datos en dos dimensiones. Posteriormente entrene un modelo de KNN (utilizando la clase implementada anteriormente) y valide que su modelo generaliza bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 5\n",
    "\n",
    "Entrene un modelo de Naive Bayes (utilizando la clase implementada anteriormente) y valide que su modelo generaliza bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
